---
layout: post
title:  "Mazzei's Blog"
paragraph_title: "On Boosting"
date:   2020-11-02 21:50:00 -0300
categories: jekyll update
---

<p align='center'><b>WHAT IS BOOSTING</b></p>
<br>

Boosting is an algorithm for building a "strong model" from "weak models". Informally defined, "strong models" refer to models that are capable of achieving through training a generalization error that is arbitrarily close to the <a href="https://en.wikipedia.org/wiki/Bayes_error_rate">Bayes error</a>, i.e. "the ultimate irreductible error" e.g. noise in the measurements that generated the data. On the contrary, "weak models" refer to models with a performance only the slightest above random.


<br><br>
<p align='center'><b>THE CASE FOR ADABOOST</b></p>
<br>

https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/chapter005.html radamacher complexity margins
https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory
Evidence Contrary to the Statistical View of Boosting
On the Dual Formulation of Boosting Algorithms

<div class="is-divider" data-content="Detailed Description Below"></div>


<p align='center'><b>BACKGROUND AND PHENOMENOLOGY FOR CLASSIFICATION</b></p>
<br>

As is customary to the mainstream phenomenology of supervised learning applied to classification, it is assumed that the training data are identical samples of a pair of random variables that have some unknown distribution and that is what allows generalization. It is finally concluded that the goal is to obtain a parametrization of such distribution. This approach used what are called "discriminative models". The conversion from probabilities to classification tags is done via a decission-theory motived operation, e.g. applying the sigmoid function.
<br>
<br>
Alternatives are discriminant functions, which enable the tagging of the input without further reference to probabilities and generative models which will, when properly modelled and calibrated, allow to both tag and generate new data. The statement and resolution of the optimization problem has it's own degenerated phenomenology (e.g. defining a probabilistic cost or a thermodynamical cost) which here wont be addressed.

<br><br>
<p align='center'><b>GENERALIZATION ERROR OF A SINGLE MODEL</b></p>
<br>


As stated in the previous section, the current approach will be that of the "discriminant models" operating over independent variables. An upper bound for the average of bounded and independent random variables generated by the same distribution can be computed using <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding's Inequality</a>, i.e.

$$\mathbf{P(\mu_m\geq \mathbf{E}[\mu_m + \epsilon]})\leq e^{-2m\epsilon^2}$$


From that it is possible to calculate<a href="https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/chapter002.html#h1-2-2">[1]</a> a lower and upper bound for the difference between the generalization error (i.e. <i>the true error or the error over the testing set</i>) and the training error for a single hypothesis. The result is (with "<i>m</i>" the size of the training set) the following, and the conclusion is that "for big enough training sets" the generalization error is the same as the training error.


$$|\epsilon_{train} - \epsilon_{true}|\leq \sqrt{\frac{ln(2e^{-2m\epsilon^2})}{2m}}$$


<br><br>
<p align='center'><b>GENERALIZATION ERROR OF A COLLECTION OF MODELS</b></p>
<br>

Building "m" datasets and training a different "weak model" on each yields "m" models. This actually depends on the particular hypothesis chosen, but the most probable case is that the space of all the possible hypotheses has infinite dimensionality. An example of that would be the existence of a continous parameter, e.g. a threshold. Noting such space as \\( \mathbf{H} \\) it can be proven <a href="https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/chapter002.html#h1-2-2">[1]</a> that:

$$\epsilon_{true} \leq \epsilon_{train} + \sqrt{\frac{32[ln(\Pi_\mathbf{H}(m))+ln(8e^{-2m\epsilon^2})]}{m}}$$

Where \\(\Pi_\mathbf{H}(m)\\) is the "growth function" and for trheshold-type functions is a polynomial in m. For \\(\mathbf{H}\\) with a VC-dimension \\(d < \infty \\) the upper bound \\(\Pi_\mathbf{H}(m)\leq (\frac{em}{d})^d\\) holds. At <a href="https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/chapter002.html#h1-2-2">[1]</a> a compression-based bound is also derived!
<br><br>

Finally assuming the existence of a target function, i.e.  assuming the existence of \\(c: \xi \rightarrow [0,1]\\) such that for any x this equality holds \\( P(y=c(x)\|x)=1 \\), and making the hypothesis that <i>it is strongly PAC learnable</i> allows to absolutely guarantee performance.





